# wwwlinks
Placeholder repo for links from www for sharing as gist or list of lists

# AI Web Research

# Laptop / PC configuration for running LLMs 
Topics covering articles, blogs and social conversations that are useful

# Resources on VRAM Requirements for LLM Inference

1. **How Much VRAM Do You Need for LLM Inference?**  
   [https://modal.com/blog/how-much-vram-need-inference](https://modal.com/blog/how-much-vram-need-inference)

2. **Demystifying VRAM Requirements for LLM Inference: Why & How**  
   [https://www.linkedin.com/pulse/demystifying-vram-requirements-llm-inference-why-how-ken-huang-cissp-rqqre](https://www.linkedin.com/pulse/demystifying-vram-requirements-llm-inference-why-how-ken-huang-cissp-rqqre)

3. **Self-Hosting LLaMA 3.1 70B (or Any 70B LLM) Affordably**  
   [https://abhinand05.medium.com/self-hosting-llama-3-1-70b-or-any-70b-llm-affordably-2bd323d72f8d/](https://abhinand05.medium.com/self-hosting-llama-3-1-70b-or-any-70b-llm-affordably-2bd323d72f8d/)

4. **Understanding VRAM and How Much Your LLM Needs**  
   [https://blog.runpod.io/understanding-vram-and-how-much-your-llm-needs/](https://blog.runpod.io/understanding-vram-and-how-much-your-llm-needs/)

5. **LLM Ops: GPU VRAM Requirements for Large Language Models**  
   [https://medium.com/@mahernaija/llm-ops-gpu-vram-requirements-for-large-language-models-llm-4eb7b827e194](https://medium.com/@mahernaija/llm-ops-gpu-vram-requirements-for-large-language-models-llm-4eb7b827e194)

## Summary
These articles cover essential topics such as:

- The VRAM requirements for running LLM inference efficiently.
- Strategies for optimizing costs while hosting large models.
- Insights into GPU VRAM needs based on model sizes and architectures.

Feel free to explore the resources above for a deeper dive into VRAM optimization techniques and cost-effective solutions for deploying large-scale LLMs.

